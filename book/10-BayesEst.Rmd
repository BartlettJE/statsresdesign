# Introduction to Bayesian Estimation

In this chapter...

## Learning objectives

1. 

To follow along to this chapter and try the code yourself, please download the data files we will be using in [this zip file](data/10_data.zip).

In this chapter, we need a few packages for the things we will cover. There is a separate appendix section (add) we have prepared to help install the `r pkg("brms")` package as it can sometimes be pretty awkward since it uses Stan and you need a C++ compiler. If you are really struggling or its very slow on your computer, brms is available on the R Studio server. See the course overview page for a link if you have never used it before. 

```{r packages, warning=FALSE, message=FALSE}
library(brms) #fitting Bayesian models
library(bayestestR) #helper functions for plotting and understanding the models
library(tidyverse)
library(see) #helper functions for plotting objects from bayestestR
library(emmeans) #Handy function for calculating (marginal) effect sizes

```

## Simple Linear Regression 

### Guided example (Schroeder & Epley, 2015)

For this guided activity, we will use data from the study by [Schroeder and Epley (2015)](https://journals.sagepub.com/stoken/default+domain/PhtK6MPtXvkgnYRrnGbA/full). We used this in the chapter 9 for the independent activity, so we will explore the data set as the guided example in this chapter to see how we can refit it as a Bayesian regression model. 

As a reminder, the aim of the study was to investigate whether delivering a short speech to a potential employer would be more effective at landing you a job than writing the speech down and the employer reading it themselves. Thirty-nine professional recruiters were randomly assigned to receive a job application speech as either a transcript for them to read or an audio recording of them reading the speech. 

The recruiters then rated the applicants on perceived intellect, their impression of the applicant, and whether they would recommend hiring the candidate. All ratings were originally on a Likert scale ranging from 0 (low intellect, impression etc.) to 10 (high impression, recommendation etc.), with the final value representing the mean across several items. 

For this example, we will focus on the hire rating (variable `r hl("Hire_Rating")`) to see whether the audio condition would lead to higher ratings than the transcript condition (variable `r hl("CONDITION")`). 

Remember the key steps of Bayesian modelling from lecture 10 (Heino et al., 2018):

1. Identify data relevant to the research question 

2. Define a descriptive model, whose parameters capture the research question

3. Specify prior probability distributions on parameters in the model 

4. Update the prior to a posterior distribution using Bayesian inference 

5. Check your model against data, and identify potential problems

#### 1. Identify data

For this example, we have the data from Schroeder and Epley, and we can label the conditions to be more intuitive. 

```{r Schroeder data,  warning=FALSE, message=FALSE}
Schroeder_data <- read_csv("data/Schroeder_hiring.csv")

# Relabel condition to be more intuitive which group is which 
Schroeder_data$CONDITION <- factor(Schroeder_data$CONDITION, 
                                   levels = c(0, 1), 
                                   labels = c("Transcript", "Audio"))
```

#### 2. Define a descriptive model

The next step is to define a descriptive model. In chapter 9, we used the `r pkg("BayesFactor")` package to use out-of-the-box tests like a t-test, but we saw in the lecture with the [Lindeloev (2019) blog post](https://lindeloev.github.io/tests-as-linear/), common statistical models are just different expressions of linear models. So, we can express the same t-test as a linear model, using `r hl("CONDITION")` as a single categorical predictor of `r hl("Hire_Rating")` as our outcome. You can enter this directly in the `r hl(brm())` function below, but its normally a good idea to clearly outline each component.  

```{r Schroeder model}

Schroeder_model1 <- bf(Hire_Rating ~ CONDITION)

```

#### 3. Specify prior probability of parameters

Once you get used to the `r pkg("brms")` package, you start to learn which priors you need for simple cases, but now we have stated a model, we can see which parameters can be assigned a prior. 

```{r}

get_prior(Schroeder_model1, # Model we defined above
          data = Schroeder_data) # Which data frame are we using? 

```

This tells us which priors we can set and what the default settings are. We have the prior, the class of prior, relevant coefficients, and the source which will all be default for now. The prior tells you what the default is. For example, there are flat uninformative priors on coefficients. When we set priors, we can either set priors for a whole class, or specific to each coefficient. With one predictor, there is only one coefficient prior to set, so it makes no difference. But when you have multiple predictors like later in chapter 10, it becomes more useful. 

The intercept and sigma are assigned student t distributions for priors. These are both pretty wide to be weak priors. 

For our example, we will use information from Schroeder and Epley. Their paper contains four studies and our data set focuses on the fourth where they apply their findings to professional recruiters. Study 1 preceded this and used students, so we can pretend we are the researchers and use this as a source of our priors for the "later" study. 

Focusing on hire rating, they found: "Evaluators who heard pitches also reported being significantly more likely to hire the candidates (*M* = 4.34, *SD* = 2.26) than did evaluators who read exactly the same pitches (*M* = 3.06, *SD* = 3.15), *t*(156) = 2.49, *p* = .01, 95% CI of the difference = [0.22, 2.34], *d* = 0.40 (see Fig. 1)". 

So, for our intercept and reference group, we can set a normally distributed prior around a mean of 3 and SD of 3 for the transcript group. Note the rounded values since these are approximations for what we expect about the measures and manipulations. 

It is normally a good idea to visualise this process to check the numbers you enter match your expectations. For the intercept, a mean and SD of 3 look like this when generating the numbers from a normal distribution:

```{r plot prior 1}

set.seed(1928) # set seed to be reproducible

hist(rnorm(100, # how many samples?
           3, # What mean? 
           3)) # What SD?

```

This turns out to be quite a weak prior since the distribution extends below 0 (which is not possible for this scale) all the way to 10 which is the upper limit of this scale. It covers pretty much the entire measurement scale with the peak around 3, so it represents a conservative estimate of what we expect the reference group to be. 

For the coefficient, the mean difference was around 1 (calculated manually by subtracting one mean from the other) and the 95% CI was quite wide from 0.22 to 2.34, so we can set a relatively weak prior expecting a normally distributed coefficient with a mean and SD of 1:.  

```{r plot prior 2}
set.seed(1928)

hist(rnorm(100, 
           1, 
           1))
```

The distribution here shows we are expecting the most likely value for the coefficient to peak around 1, but it could span from -1 (transcript to be higher than audio) to around 3 (audio to be much higher than transcript). 

Now we have our priors, we can save them to a new object:

```{r Schroeder prior}
prior <- set_prior("normal(1, 1)", class = "b") + 
  set_prior("normal(3, 3)", class = "Intercept")

```

::: {.info data-latex=""}
Remember it is important to check the sensitivity of the results to the choice of prior. So, once we're finished, we will check how stable the results are to an uninformative prior, keeping the defaults.
:::

#### 4. Update the prior to the posterior

This is going to be the longest section as we are going to fit the `brms` model and then explore the posterior. 

As the process relies on sampling using MCMC, it is important to set a seed within the function for reproducibility, so the semi-random numbers have a consistent starting point. This might take a while depending on your computer, then you will get a bunch of output for fitting the model and sampling from the MCMC chains. 

```{r Schroeder fit}

Schroeder_fit <- brm(
  formula = Schroeder_model1, # formula we defined above 
  data = Schroeder_data, # Data frame we're using 
  family = gaussian(), # What distribution family do we want for the likelihood function? Many examples we use in psychology are Gaussian, but check the documentation for options
  prior = prior, # priors we stated above
  sample_prior = TRUE, # Setting this to true includes the prior in the object, so we can include it on plots later
  seed = 1908
)

```

::: {.info data-latex=""}
Depending on your computer, this can take a while. There is an argument called file which you can create a character string for the object to be saved in. This is super handy for more complex models as you can simply reload the results instead of waiting for it to rerun.
:::

There will be a lot of output here to explain the fitting and sampling process. See the references for explanations of how MCMC sampling works, but for a quick overview, we want to sample from the posterior distribution based on the data and model. The default of `brms` is to sample from four chains, with each chain containing 2000 iterations (1000 of which are warmup / burnin iterations). If you get warning messages about model fit or convergence issues, you can increase the number of iterations. This becomes more important with more complex models, so all the defaults should be fine for the relatively simple models we fit in this chapter. We will return to chains and convergence when we see the trace plots later. 

Now we have fitted the model, we can also double check the priors you set are what you wanted. You will see the source for the priors you set switched from default to user. 

```{r check user priors}

prior_summary(Schroeder_fit)

```

Now we have our model, we can get a model summary like any old linear model. 

```{r Schroeder posterior}
summary(Schroeder_fit)

```

At the top, we have information on the model fitting process, like the family, data, and draws from the posterior summarising the chain iterations. 

Population-level effects is our main area of interest. This is where we have the posterior probability distribution summary statistics. We will look at the whole distribution soon, but for now, we can see the median point-estimate for the intercept is 2.94 with a 95% credible interval between 1.93 and 3.93. This is what we expect the mean of the reference group to be, i.e., the transcript group. 

We then have the median coefficient of 1.71 with a 95% credible interval between 0.37 and 2.99. This means our best guess for the mean difference / slope is an increase of 1.71 for the audio group. 

For convergence issues, if Rhat is different from 1, it can suggest there are problems with the model fitting process. You can also look at the effective sample size statistics (the columns ending in ESS), but we did not explore this in the lecture. 

For a tidier summary of the parameters, we can also use the handy `r hl(describe_posterior())` function from `r pkg("bayestestR")`. 

```{r Schroeder describe posterior}

describe_posterior(Schroeder_fit)

```

We can use this as a way to create ROPE regions for the effects and it tells us useful things like the probability of direction for the effect. 

This will be more useful when it comes to comparing models and building multiple regression models, but there is a specific function to get the model $R^2$ and its 95% credible interval. This tells you the proportion of variance in your outcome your predictor(s) explain. 

```{r Schroeder R2}

bayes_R2(Schroeder_fit)

```

Until now, we have focused on point-estimates and intervals of the posterior, but the main strength of Bayesian statistics is summarising the parameters as a whole posterior probability distribution, so we will now turn to the various plotting options. 

The first plot is useful for seeing the posterior of each parameter and the trace plots to check on any convergence issues. 

```{r Schroeder parameters and trace, warning=FALSE, message=FALSE}

plot(Schroeder_fit)

```

For this model, we have three plots: one for the intercept, one for the coefficient/slope, and one for sigma. On the left, we have the posterior probability distributions for each. On the right, we have trace plots. By default, `brms` uses four chains - or series of samples using MCMC - and this shows how each chain moves around the parameter space. Essentially, we want the trace plots to look like fuzzy caterpillars with a random series of lines. If there are spike which deviate massively from the rest, or the lines get stuck in one area, this suggests there are convergence issues. 

These plots are useful for an initial feel of the parameter posteriors, but there are a great series of functions from the `r pkg("bayestestR")` package which you can use on their own, or wrap them in the `r hl(plot())` function after loading the `r pkg("see")` package. For example, we can see an overlay of the prior and posterior for the main parameters of interest. On its own, `r hl(p_direction())` tells you the probability of direction for each parameter, i.e., how much of the distribution is above or below 0? Wrapped in `r hl(plot())`, you can see the prior and posterior, with the posterior divided in areas above or below 0. 

```{r Schroeder p direction, message=FALSE, warning=FALSE}

plot(p_direction(Schroeder_fit), 
     priors = TRUE) 

```

::: {.warning data-latex=""}
For this to work, you must specify priors in brms. It does not work with coefficient defaults. 
:::

We can see the pretty wide prior in blue, then the posterior. Almost all of the posterior distribution is above zero to show we're pretty confident that audio is associated with higher hire ratings than transcript. 

The next useful plot is seeing the 95% HDI / credible interval. On its own, `r hl(hdi())` will show you the 95% HDI for your parameters. Wrapped in `r hl(plot())`, you can visualise the HDI compared to zero for your main parameters. If the HDI excludes zero, you can be confident in a positive or negative effect, at least conditional on these data and model. Remember, there is a difference between the small world and big world of models. This is not the absolute truth, just the most credible values conditioned on our data and model. 

```{r Schroeder HDI, warning=FALSE, message=FALSE}

plot(hdi(Schroeder_fit))

```

For this example, the 95% HDI excludes 0, so we can be confident the coefficient posterior is a positive effect, with the audio group leading to higher hire ratings than the transcript group. 

Finally, we might not be interested in comparing the coefficients to a point-value of 0, we might have a stronger level of evidence in mind, where the coefficient must exclude a range of values in the ROPE process we explored in chapter 9. For example, maybe effects smaller than 1 unit difference are too small to be practically/theoretically meaningful. 

::: {.info data-latex=""}
Remember this is potentially the most difficult decision to make, maybe more so than choosing priors. Many areas of psychology do not have clear guidelines/expectations for smallest effect sizes of interest, so it is down to you to explain and justify your approach based on your understanding of the topic area.
:::

```{r Schroeder ROPE, warning=FALSE, message=FALSE}
plot(rope(Schroeder_fit, 
          range = c(-1, 1))) # What is the ROPE range for your smallest effects of interest? 
```

For this example, for a sample size of 39, we have pretty strong evidence in favour of a positive effect in the audio group. The 95% HDI excludes zero, but if we set a ROPE of 1 unit, we do not quite exclude it. This means if we wanted to be more confident that the effect exceeded the ROPE, we would need more data. This is just for demonstration purposes, I'm not sure if the original study would consider an effect of 1 as practically meaningful, or whether they would just be happy with any non-zero effect.

Following from chapter 9, we saw we can also use Bayesian statistics to test hypotheses. This works in a modelling approach as `brms` has a function to test hypotheses. We must provide the fitted model object and state a hypothesis to test. This relies on a character description of the parameter and test value. For a full explanation, see the [brms documentation](https://paul-buerkner.github.io/brms/reference/hypothesis.html) for the function. Here, we will test the coefficient/slope against a point-null of 0. 

```{r Schroeder hypothesis}

hypothesis(Schroeder_fit, # brms model we fitted earlier
           hypothesis = "CONDITIONAudio = 0") 

```

::: {.info data-latex=""}
We must state a character hypothesis which requires you to select a parameter. Here, we focus on the `r hl("CONDITIONAudio")` parameter, i.e., our slope, which must match the name in the model. We can then state values to test against, like here against a point-null of 0 for a Bayes factor. Alternatively, you can test posterior odds where you compare masses of the posterior like CONDITIONAudio > 0.
:::

The key part of the output is the evidence ratio, but we also have the estimate and 95% credible interval. As we are testing a point-null of 0, we are testing the null hypothesis against the alternative of a non-null effect. As the value is below 1, it suggests we have evidence in favour of the alternative compared to the null. I prefer to express things above 1 as its easier to interpret. You can do this by dividing 1 by the ratio, which should provide a Bayes factor of 5.56. 

Alternatively, you can calculate the posterior odds by stating regions of the posterior to test. For example, if we used "CONDITIONAudio > 0", this would provide a ratio of the posterior probability of positive effects above 0 to the posterior probability of negative effects below 0. For this example, this would be a posterior odds of 128 in favour of positive effects. Note, when all the posterior is above 0, you can get a result of Inf (infinity) as all the evidence is in favour of positive effects.

#### 5. Model checking 

Finally, we have our model checking procedure. We already looked at some information for this such as Rhat and the trace plots. This suggests the model fitted OK. We also want to check the model reflects the properties of the data. This does not mean we want it exactly the same and overfit to the data, but it should follow a similar pattern to show our model captures the features of the data. 

Bayesian models are generative, which means once they are fitted, we can use them to sample values from the posterior and make predictions from it. One key process is called a posterior predictive check which takes the model and uses is to generate new samples. This shows how you have conditioned the model and what it expects. 

The plot below is a `r pkh("brms")` function for facilitating this. The thick blue line is your data for the outcome. The light blue lines are 100 samples from the posterior to show what the model expects about the outcome. 

```{r Schroeder model check, warning=FALSE, message=FALSE}

pp_check(Schroeder_fit, 
         ndraws = 100) # How many draws from the posterior? Higher values means more lines

```

For this example, it does an OK job at capturing the pattern of data and the bulk of the observed data follows the generated curves. However, you can see the data are quite flat compared to the predicted values. As we expect a Gaussian distribution, the model will happily produce normal curves. The model also happily expects values beyond the range of data as our scale is bound to 0 and 10. This is hugely common in psychological research as we expect Gaussian distributions from ordinal bound data. So, while this model does an OK job, we could potentially improve it by focusing on an ordinal regression model so we can factor in the bounded nature of the measure. 

::: {.try data-latex=""}
If you want to challenge yourself, I recommend working through [Bürkner and Vuorre (2019)](https://doi.org/10.1177/2515245918823199) and applying your understanding to this task. This is going to be a common theme in the examples you see in the independent activities as psychology articles (myself included) often use metric models to analyse arguably ordinal data.
:::

### Independent activity (Brandt et al., 2014)

For an independent activity, we will use data from the study by [Brandt et al. (2014)](https://econtent.hogrefe.com/doi/full/10.1027/1864-9335/a000191). The aim of Brandt et al. was to replicate a relatively famous social psychology study on the effect of recalling unethical behaviour on the perception of brightness. 

In common language, unethical behaviour is considered as "dark", so the original authors designed a priming experiment where participants were randomly allocated to recall an unethical behaviour or an ethical behaviour from their past. Participants then completed a series of measures including their perception of how bright the testing room was. Brandt et al. were sceptical and wanted to replicate this study to see if they could find similar results. 

Participants were randomly allocated (`r hl("ExpCond")`) to recall an unethical behaviour (n = 49) or an ethical behaviour (n = 51). The key outcome is their perception of how bright the room was (`r hl("WellLitSca")`), from 1 (not bright at all) to 7 (very bright). The research question was: Does recalling unethical behaviour lead people to perceive a room as darker than if they recall ethical behaviour? 

Use your understanding of the design to address the research question. If you follow the link to Brandt et al. above, the means and standard deviations of the original study are included in Table 2. This might be useful for thinking about your priors, but keep in mind how sensitive your conclusions are to your choice of prior. 

```{r Brandt data,  warning=FALSE, message=FALSE}
Brandt_data <- read_csv("data/Brandt_unlit.csv")

# Recode to dummy coding 
Brandt_data <- Brandt_data %>% 
  mutate(ExpCond = case_when(ExpCond == 1 ~ 0,
                             ExpCond == -1 ~ 1))

# Relabel condition to be more intuitive which group is which 
# Ethical is the reference group
Brandt_data$ExpCond <- factor(Brandt_data$ExpCond, 
                                   levels = c(0, 1), 
                                   labels = c("Ethical", "Unethical"))
```

::: {.try data-latex=""}
From here, apply what you learnt in the first guided example to this new independent activity. 
:::

```{r Brandt model}

Brandt_model1 <- NULL

```

## Multiple Linear Regression 

### Guided example (Heino et al., 2018)

#### 1. Identify data

For the second guided example we covered in the lecture, we will explore the model included in [Heino et al. (2018)](https://doi.org/10.1080/21642850.2018.1428102) for their Bayesian data analysis tutorial. They explored the feasibility and acceptability of the ”Let’s Move It” intervention to increase physical activity in 43 older adolescents. 

They randomised participants into two groups (`r hl("intervention")`) for control (0) and intervention (1) arms (group sessions on motivation and self-regulation skills, and teacher training). Their outcome was a measure of autonomous motivation (`r hl("value")`) on a 1-5 scale, with higher values meaning greater motivation. They measured the outcome at both baseline (0) and six weeks after (1; `r hl("time")`).

Their research question was: To what extent does the intervention affect autonomous motivation? 

```{r Heino data, warning=FALSE, message=FALSE}

Heino_data <- read_csv("data/Heino-2018.csv") %>% 
  group_by(ID, intervention, time) %>% 
  summarise(value = mean(value, na.rm = TRUE)) %>% 
  ungroup()

```

::: {.info data-latex=""}
Part of their tutorial discusses a bigger multilevel model considering different scenarios, but for this demonstration, we're just averaging over the scenarios to get the mean motivation.
:::

#### 2. Define a descriptive model

I recommend reading the article as they explain this process in more detail. We essentially have an outcome of autonomous motivation (`r hl("value")`) and we want to look at the interaction between `r hl("intervention")` and `r hl("time")`. They define a fixed intercept in the model with the `1 +` part. Its also technically a multi-level model as they define a random intercept for each participant (`(1 | ID)`) to ensure we recognise time is within-subjects. 

::: {.info data-latex=""}
By default, R includes a fixed intercept in the model, so you would get the same results without adding it to the model. However, people often include it so it is explicit in the model formula.
:::

```{r Heino model}

Heino_model <- bf(value ~ 1 + time * intervention + (1 | ID))

```

#### 3. Specify prior probability of parameters

This is another place where I recommend reading the original article for more information. They discuss their choices and essentially settle on wide weak priors for the coefficients to say small effects are more likely but they allow larger effects. The two standard deviation classes are then assigned relatively wide Cauchy priors to only allow positive values. 

```{r Heino priors}
get_prior(Heino_model, data = Heino_data)

Heino_priors <- prior(normal(0, 5), class = "b") +
  prior(cauchy(0, 1), class = "sd") +
  prior(cauchy(0, 2), class = "sigma")

```

Note you get a warning about missing data but since its a multi-level model, we just have fewer observations in some conditions instead of the whole case being removed. 

#### 4. Update prior to posterior

This is going to be the longest section as we are going to fit the `brms` model and then explore the posterior. 

As the process relies on sampling using MCMC, it is important to set a seed for reproducibility, so the semi-random numbers have a consistent starting point. This might take a while depending on your computer, then you will get a bunch of output for fitting the model and sampling from the MCMC chains. 

```{r Heino fit}
set.seed(78912) # For reproducibility

Heino_fit <- brm(
  formula = Heino_model,
  data = Heino_data,
  prior = Heino_priors,
  family = gaussian(),
  seed = 2108
)

```

Now we have fitted the model, let's have a look at the summary. 

```{r Heino summary}
summary(Heino_fit)

```

The model summary is very similar to the examples in the simple linear regression section, but we also have a new section for group-level effects since we added a random intercept for participants.

Exploring the coefficients, all the effects are pretty small, with the largest effect being 0.09 units. There is quite a bit of uncertainty here, with 95% credible intervals spanning negative and positive effects. 

This is a start, but particularly in more complicated models like this, plotting is going to be your best friend for understanding what is going on. 

```{r Heino plots}

plot(Heino_fit)

plot(p_direction(Heino_fit), 
     priors = TRUE) # plot the priors

plot(hdi(Heino_fit))

```

Regardless of the output we look at here, there is not much going on across any of the predictors. The data comes from a feasibility study, so the sample size was pretty small and its mainly about how receptive participants are to the intervention. 

As a bonus extra, you can also use the `r pkg("emmeans")` package to calculate marginal effects on the posterior distribution. Its not important here as the interaction is not telling us anything, but it might come in handy in future. 

```{r Heino marginal}

(Heino_means <- emmeans(Heino_fit, # add the model object  
        ~ time | intervention)) # We want to separate time by levels of intervention

```

This provides the median value of the posterior for the combination of time and intervention. Here, we can see pretty clearly there is not much going on, with very little difference across the estimates and all the 95% credible intervals overlapping. 

Depending on how you want to express the marginal means, you can also use the emmeans object to calculate contrasts, expressing the effects as mean differences in the posterior for each group/condition.  

```{r Heino contrasts}

contrast(Heino_means)

```

#### 5. Model check

As the final step, we can look at the posterior predictive check to make sure the model is capturing the features of the data. Compared to the first guided example, the model maps onto the data quite well, with the samples following the underlying data quite well. We are still using metric models to analyse ultimately ordinal data (despite calculating the mean response), so the expected values go beyond the range of data (1-5).

```{r Heino pp check}
pp_check(Heino_fit,
         ndraws = 100) # 100 draws from the model
```

::: {.try data-latex=""}
If you scroll to the end of the Heino et al. article, they demonstrate how you can fit an ordinal model to the data. 
:::

### Independent activity (Coleman et al., 2014)

For an independent activity, we will use data from the study by [Coleman et al. (2014)](https://psyarxiv.com/k5fp8/). Coleman et al. contains two studies investigating religious mystical experiences. One study focused on undergraduates and a second study focused on experienced meditators who were part of a unique religious group.  

The data set contains a range of variables used for the full model in the paper. We are going to focus on a small part of it for this exercise, but feel free to explore developing the full model as was used in study 1. The key variables are: 

1. `r hl("Age")` - Measured in years

2. `r hl("Gender")` - 0 = male; 1 = female

3. `r hl("Week_med")` - Ordinal measure of how often people meditate per week, with higher values meaning more often

4. `r hl("Time_session")` - Ordinal measure of how long people meditate per session, with higher values meaning longer

5. `r hl("Absorption_SUM")` - Sum score of the Modified Tellegen Absorption scale, with higher values meaning greater trait levels of imaginative engagement 

6. `r hl("EQ_SUM")` - Sum score of the Empathizing Quotient short form, with higher values meaning greater theory of mind ability 

7. `r hl("Mscale_SUM")` - Sum score of the Hood M-scale, with higher values meaning more self-reported mystical experiences

Previous studies had explored these components separately and mainly in undergraduates, so Coleman et al. took the opportunity to explore a unique sample of a highly committed religious group. The final model included all seven variables, but for this example, we will just focus on absorption (`r hl("Absorption_SUM")`) and mentalizing (`r hl("EQ_SUM")`) as they were the main contributors, with the other variables as covariates.

Our research question is: How do absorption (`r hl("Absorption_SUM")`) and mentalizing (`r hl("EQ_SUM")`) affect mystical experiences (`r hl("Mscale_SUM")`) as an outcome? Focus on entering the two variables as individual predictors at first, then explore an interaction. 

Use your understanding of the design to address the research question. If you follow the link to Coleman et al. above, you can see the results of study 2 which focused on undergraduate students. This study is presented second, but you can use it for this example to develop your understanding of the measures for your priors. Think about whether you have weaker or stronger priors depending on your understanding of the topic, but keep in mind how sensitive your conclusions are to your choice of prior. 

```{r Coleman data}

Coleman_data <- read_csv("data/Coleman_2019.csv")

```

::: {.try data-latex=""}
From here, apply what you learnt in the first guided example to this new independent task. 
:::

```{r Coleman model}

Coleman_model <- NULL

```

